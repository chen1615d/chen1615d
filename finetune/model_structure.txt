model.ckpt["gene"]
1****************************************************************************************************
Gene keys: dict_keys(['config', 'state_dict'])
2****************************************************************************************************
Gene Config: {'config_version': 1.0, 'training': {'trainer': 'lightning', 'seed': 326877, 'experiment_name': 'run', 'max_updates': 22000, 'max_epochs': 20, 'log_interval': 100, 'logger_level': 'info', 'log_format': 'simple', 'log_detailed_config': False, 'should_not_log': False, 'colored_logs': True, 'tensorboard': False, 'cudnn_benchmark': False, 'wandb': {'enabled': False, 'entity': None, 'project': 'mmf', 'name': 'run', 'log_checkpoint': False}, 'batch_size': 128, 'batch_size_per_device': None, 'update_frequency': 1, 'num_workers': 1, 'fast_read': False, 'dataset_size_proportional_sampling': True, 'pin_memory': False, 'persistent_workers': True, 'checkpoint_interval': 1000, 'evaluation_interval': 1000, 'clip_gradients': True, 'clip_norm_mode': 'all', 'early_stop': {'enabled': True, 'patience': 999999999999999, 'criteria': 'rnaseq/loss_getter', 'minimize': True}, 'lr_scheduler': True, 'lr_steps': [], 'lr_ratio': 0.1, 'use_warmup': True, 'warmup_factor': 0.2, 'warmup_iterations': 100, 'device': 'cuda', 'local_rank': None, 'verbose_dump': False, 'find_unused_parameters': False, 'evaluate_metrics': False, 'detect_anomaly': False, 'fp16': False, 'callbacks': [{'type': 'summary_writer', 'params': {'foo': 'bar'}}], 'exit_on_nan_losses': True, 'max_grad_l2_norm': 0.25, 'prefetch_factor': 200, 'task_size_proportional_sampling': True, 'max_steps': 400000, 'run_type': 'train_inference', 'timeout': 300}, 'trainer': {'type': 'lightning', 'params': {'num_nodes': 64, 'precision': 'bf16', 'deterministic': False, 'benchmark': False, 'gradient_clip_val': 1, 'num_sanity_val_steps': 0, 'enable_checkpointing': True, 'accumulate_grad_batches': 8, 'val_check_interval': 15625, 'log_every_n_steps': 100, 'logger': False, 'limit_val_batches': 1.0, 'enable_progress_bar': True, 'accelerator': 'gpu', 'devices': 1, 'strategy': 'ddp_find_unused_parameters_false', 'strategy_params': {'zero_optimization': True, 'stage': 2}}}, 'evaluation': {'metrics': [{'type': 'loss_getter', 'params': {'get_name': 'mse'}, 'saveby': 'min', 'topk': 1}], 'use_cpu': False, 'predict': False, 'predict_file_format': 'pickle', 'reporter': {'type': 'file', 'params': {}}}, 'model_config': {'mae_autobin': {'mask_gene_name': False, 'gene_num': 19266, 'seq_len': 19266, 'encoder': {'hidden_dim': 768, 'depth': 12, 'heads': 12, 'dim_head': 64, 'seq_len': 19266, 'module_type': 'transformer', 'norm_first': False}, 'decoder': {'hidden_dim': 512, 'depth': 6, 'heads': 8, 'dim_head': 64, 'module_type': 'performer', 'seq_len': 19266, 'norm_first': False}, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102, 'bin_num': 100, 'bin_alpha': 1.0, 'rawcount': True, 'model': 'mae_autobin'}}, 'dataset_config': {'rnaseq': {'test_valid_train_idx_dict': '/nfs_beijing/minsheng/data/os10000w-new/global_shuffle/meta.csv.train_set_idx_dict.pt', 'valid_data_path': '/nfs_beijing/minsheng/data/valid_count_10w.npz', 'num_tokens': 13, 'train_data_path': None, 'isPanA': False, 'isPlanA1': False, 'max_files_to_load': 5, 'bin_type': 'auto_bin', 'bin_num': 100, 'value_mask_prob': 0.3, 'zero_mask_prob': 0.03, 'replace_prob': 0.8, 'random_token_prob': 0.1, 'mask_ignore_token_ids': [0], 'decoder_add_zero': True, 'mae_encoder_max_seq_len': 15000, 'isPlanA': False, 'mask_prob': 0.3, 'seq_len': 19266, 'rawcount': True, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102}}, 'datasets': 'rnaseq', 'model': 'mae_autobin', 'config': 'projects/mae_autobin/resolution.yaml', 'run_type': 'train_inference', 'optimizer': {'allow_unused_parameters': False, 'enable_state_sharding': False, 'params': {'eps': 1e-08, 'lr': 0.0001, 'weight_decay': 0}, 'type': 'adam_w'}, 'scheduler': {'type': 'cosine_annealying_warmup_restarts', 'interval': 'step', 'params': {'first_cycle_steps': 976600, 'cycle_mult': 2, 'max_lr': 0.0001, 'min_lr': 1e-08, 'warmup_steps': 9766, 'gamma': 0.9}}, 'env': {'cache_dir': '/nfs_beijing/kubeflow-user/minsheng_2022/.cache/torch/mmf', 'dataset_zoo': 'configs/zoo/datasets.yaml', 'model_zoo': 'configs/zoo/models.yaml', 'data_dir': '/nfs_beijing/kubeflow-user/minsheng_2022/.cache/torch/mmf/data', 'save_dir': '/nfs_beijing/minsheng/gitlab/mmf/results/0.1B/0.1B-trans-pGAU-shuffle5-autobin100-mask0.3-bts1024-0226-bin100-k8s-lr1e-4-resume', 'log_dir': '', 'report_dir': '', 'tensorboard_logdir': '', 'wandb_logdir': '', 'user_dir': ''}, 'distributed': {'init_method': None, 'rank': 0, 'port': -1, 'backend': 'nccl', 'world_size': 1, 'no_spawn': False}, 'checkpoint': {'resume': False, 'resume_file': '/nfs_beijing/minsheng/gitlab/mmf/results/0.1B/0.1B-trans-pGAU-shuffle5-autobin100-mask0.3-bts1024-0226-bin100-k8s-lr1e-4/current.ckpt', 'resume_best': False, 'resume_pretrained': False, 'resume_zoo': None, 'zoo_config_override': False, 'pretrained_state_mapping': {}, 'max_to_keep': -1, 'save_git_details': False, 'reset': {'all': False, 'optimizer': False, 'counts': False, 'fp16_scaler': False}}, 'multitasking': {'enabled': True, 'type': 'size_proportional', 'params': {}}, 'start_rank': 0, 'device_id': 0}
3****************************************************************************************************
Gene State Dict Keys: odict_keys(['model.token_emb.mlp.weight', 'model.token_emb.mlp.bias', 'model.token_emb.mlp2.weight', 'model.token_emb.mlp2.bias', 'model.token_emb.emb.weight', 'model.token_emb.emb_mask.weight', 'model.token_emb.emb_pad.weight', 'model.pos_emb.weight', 'model.decoder_embed.weight', 'model.decoder_embed.bias', 'model.norm.weight', 'model.norm.bias', 'model.to_final.weight', 'model.to_final.bias', 'model.encoder.transformer_encoder.0.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.0.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.0.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.0.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.0.linear1.weight', 'model.encoder.transformer_encoder.0.linear1.bias', 'model.encoder.transformer_encoder.0.linear2.weight', 'model.encoder.transformer_encoder.0.linear2.bias', 'model.encoder.transformer_encoder.0.norm1.weight', 'model.encoder.transformer_encoder.0.norm1.bias', 'model.encoder.transformer_encoder.0.norm2.weight', 'model.encoder.transformer_encoder.0.norm2.bias', 'model.encoder.transformer_encoder.1.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.1.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.1.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.1.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.1.linear1.weight', 'model.encoder.transformer_encoder.1.linear1.bias', 'model.encoder.transformer_encoder.1.linear2.weight', 'model.encoder.transformer_encoder.1.linear2.bias', 'model.encoder.transformer_encoder.1.norm1.weight', 'model.encoder.transformer_encoder.1.norm1.bias', 'model.encoder.transformer_encoder.1.norm2.weight', 'model.encoder.transformer_encoder.1.norm2.bias', 'model.encoder.transformer_encoder.2.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.2.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.2.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.2.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.2.linear1.weight', 'model.encoder.transformer_encoder.2.linear1.bias', 'model.encoder.transformer_encoder.2.linear2.weight', 'model.encoder.transformer_encoder.2.linear2.bias', 'model.encoder.transformer_encoder.2.norm1.weight', 'model.encoder.transformer_encoder.2.norm1.bias', 'model.encoder.transformer_encoder.2.norm2.weight', 'model.encoder.transformer_encoder.2.norm2.bias', 'model.encoder.transformer_encoder.3.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.3.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.3.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.3.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.3.linear1.weight', 'model.encoder.transformer_encoder.3.linear1.bias', 'model.encoder.transformer_encoder.3.linear2.weight', 'model.encoder.transformer_encoder.3.linear2.bias', 'model.encoder.transformer_encoder.3.norm1.weight', 'model.encoder.transformer_encoder.3.norm1.bias', 'model.encoder.transformer_encoder.3.norm2.weight', 'model.encoder.transformer_encoder.3.norm2.bias', 'model.encoder.transformer_encoder.4.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.4.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.4.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.4.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.4.linear1.weight', 'model.encoder.transformer_encoder.4.linear1.bias', 'model.encoder.transformer_encoder.4.linear2.weight', 'model.encoder.transformer_encoder.4.linear2.bias', 'model.encoder.transformer_encoder.4.norm1.weight', 'model.encoder.transformer_encoder.4.norm1.bias', 'model.encoder.transformer_encoder.4.norm2.weight', 'model.encoder.transformer_encoder.4.norm2.bias', 'model.encoder.transformer_encoder.5.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.5.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.5.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.5.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.5.linear1.weight', 'model.encoder.transformer_encoder.5.linear1.bias', 'model.encoder.transformer_encoder.5.linear2.weight', 'model.encoder.transformer_encoder.5.linear2.bias', 'model.encoder.transformer_encoder.5.norm1.weight', 'model.encoder.transformer_encoder.5.norm1.bias', 'model.encoder.transformer_encoder.5.norm2.weight', 'model.encoder.transformer_encoder.5.norm2.bias', 'model.encoder.transformer_encoder.6.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.6.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.6.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.6.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.6.linear1.weight', 'model.encoder.transformer_encoder.6.linear1.bias', 'model.encoder.transformer_encoder.6.linear2.weight', 'model.encoder.transformer_encoder.6.linear2.bias', 'model.encoder.transformer_encoder.6.norm1.weight', 'model.encoder.transformer_encoder.6.norm1.bias', 'model.encoder.transformer_encoder.6.norm2.weight', 'model.encoder.transformer_encoder.6.norm2.bias', 'model.encoder.transformer_encoder.7.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.7.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.7.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.7.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.7.linear1.weight', 'model.encoder.transformer_encoder.7.linear1.bias', 'model.encoder.transformer_encoder.7.linear2.weight', 'model.encoder.transformer_encoder.7.linear2.bias', 'model.encoder.transformer_encoder.7.norm1.weight', 'model.encoder.transformer_encoder.7.norm1.bias', 'model.encoder.transformer_encoder.7.norm2.weight', 'model.encoder.transformer_encoder.7.norm2.bias', 'model.encoder.transformer_encoder.8.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.8.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.8.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.8.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.8.linear1.weight', 'model.encoder.transformer_encoder.8.linear1.bias', 'model.encoder.transformer_encoder.8.linear2.weight', 'model.encoder.transformer_encoder.8.linear2.bias', 'model.encoder.transformer_encoder.8.norm1.weight', 'model.encoder.transformer_encoder.8.norm1.bias', 'model.encoder.transformer_encoder.8.norm2.weight', 'model.encoder.transformer_encoder.8.norm2.bias', 'model.encoder.transformer_encoder.9.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.9.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.9.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.9.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.9.linear1.weight', 'model.encoder.transformer_encoder.9.linear1.bias', 'model.encoder.transformer_encoder.9.linear2.weight', 'model.encoder.transformer_encoder.9.linear2.bias', 'model.encoder.transformer_encoder.9.norm1.weight', 'model.encoder.transformer_encoder.9.norm1.bias', 'model.encoder.transformer_encoder.9.norm2.weight', 'model.encoder.transformer_encoder.9.norm2.bias', 'model.encoder.transformer_encoder.10.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.10.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.10.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.10.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.10.linear1.weight', 'model.encoder.transformer_encoder.10.linear1.bias', 'model.encoder.transformer_encoder.10.linear2.weight', 'model.encoder.transformer_encoder.10.linear2.bias', 'model.encoder.transformer_encoder.10.norm1.weight', 'model.encoder.transformer_encoder.10.norm1.bias', 'model.encoder.transformer_encoder.10.norm2.weight', 'model.encoder.transformer_encoder.10.norm2.bias', 'model.encoder.transformer_encoder.11.self_attn.in_proj_weight', 'model.encoder.transformer_encoder.11.self_attn.in_proj_bias', 'model.encoder.transformer_encoder.11.self_attn.out_proj.weight', 'model.encoder.transformer_encoder.11.self_attn.out_proj.bias', 'model.encoder.transformer_encoder.11.linear1.weight', 'model.encoder.transformer_encoder.11.linear1.bias', 'model.encoder.transformer_encoder.11.linear2.weight', 'model.encoder.transformer_encoder.11.linear2.bias', 'model.encoder.transformer_encoder.11.norm1.weight', 'model.encoder.transformer_encoder.11.norm1.bias', 'model.encoder.transformer_encoder.11.norm2.weight', 'model.encoder.transformer_encoder.11.norm2.bias', 'model.encoder.norm.weight', 'model.encoder.norm.bias', 'model.decoder.performer.calls_since_last_redraw', 'model.decoder.performer.net.layers.0.0.norm.weight', 'model.decoder.performer.net.layers.0.0.norm.bias', 'model.decoder.performer.net.layers.0.0.fn.fast_attention.projection_matrix', 'model.decoder.performer.net.layers.0.0.fn.to_q.weight', 'model.decoder.performer.net.layers.0.0.fn.to_q.bias', 'model.decoder.performer.net.layers.0.0.fn.to_k.weight', 'model.decoder.performer.net.layers.0.0.fn.to_k.bias', 'model.decoder.performer.net.layers.0.0.fn.to_v.weight', 'model.decoder.performer.net.layers.0.0.fn.to_v.bias', 'model.decoder.performer.net.layers.0.0.fn.to_out.weight', 'model.decoder.performer.net.layers.0.0.fn.to_out.bias', 'model.decoder.performer.net.layers.0.1.norm.weight', 'model.decoder.performer.net.layers.0.1.norm.bias', 'model.decoder.performer.net.layers.0.1.fn.fn.w1.weight', 'model.decoder.performer.net.layers.0.1.fn.fn.w1.bias', 'model.decoder.performer.net.layers.0.1.fn.fn.w2.weight', 'model.decoder.performer.net.layers.0.1.fn.fn.w2.bias', 'model.decoder.performer.net.layers.1.0.norm.weight', 'model.decoder.performer.net.layers.1.0.norm.bias', 'model.decoder.performer.net.layers.1.0.fn.fast_attention.projection_matrix', 'model.decoder.performer.net.layers.1.0.fn.to_q.weight', 'model.decoder.performer.net.layers.1.0.fn.to_q.bias', 'model.decoder.performer.net.layers.1.0.fn.to_k.weight', 'model.decoder.performer.net.layers.1.0.fn.to_k.bias', 'model.decoder.performer.net.layers.1.0.fn.to_v.weight', 'model.decoder.performer.net.layers.1.0.fn.to_v.bias', 'model.decoder.performer.net.layers.1.0.fn.to_out.weight', 'model.decoder.performer.net.layers.1.0.fn.to_out.bias', 'model.decoder.performer.net.layers.1.1.norm.weight', 'model.decoder.performer.net.layers.1.1.norm.bias', 'model.decoder.performer.net.layers.1.1.fn.fn.w1.weight', 'model.decoder.performer.net.layers.1.1.fn.fn.w1.bias', 'model.decoder.performer.net.layers.1.1.fn.fn.w2.weight', 'model.decoder.performer.net.layers.1.1.fn.fn.w2.bias', 'model.decoder.performer.net.layers.2.0.norm.weight', 'model.decoder.performer.net.layers.2.0.norm.bias', 'model.decoder.performer.net.layers.2.0.fn.fast_attention.projection_matrix', 'model.decoder.performer.net.layers.2.0.fn.to_q.weight', 'model.decoder.performer.net.layers.2.0.fn.to_q.bias', 'model.decoder.performer.net.layers.2.0.fn.to_k.weight', 'model.decoder.performer.net.layers.2.0.fn.to_k.bias', 'model.decoder.performer.net.layers.2.0.fn.to_v.weight', 'model.decoder.performer.net.layers.2.0.fn.to_v.bias', 'model.decoder.performer.net.layers.2.0.fn.to_out.weight', 'model.decoder.performer.net.layers.2.0.fn.to_out.bias', 'model.decoder.performer.net.layers.2.1.norm.weight', 'model.decoder.performer.net.layers.2.1.norm.bias', 'model.decoder.performer.net.layers.2.1.fn.fn.w1.weight', 'model.decoder.performer.net.layers.2.1.fn.fn.w1.bias', 'model.decoder.performer.net.layers.2.1.fn.fn.w2.weight', 'model.decoder.performer.net.layers.2.1.fn.fn.w2.bias', 'model.decoder.performer.net.layers.3.0.norm.weight', 'model.decoder.performer.net.layers.3.0.norm.bias', 'model.decoder.performer.net.layers.3.0.fn.fast_attention.projection_matrix', 'model.decoder.performer.net.layers.3.0.fn.to_q.weight', 'model.decoder.performer.net.layers.3.0.fn.to_q.bias', 'model.decoder.performer.net.layers.3.0.fn.to_k.weight', 'model.decoder.performer.net.layers.3.0.fn.to_k.bias', 'model.decoder.performer.net.layers.3.0.fn.to_v.weight', 'model.decoder.performer.net.layers.3.0.fn.to_v.bias', 'model.decoder.performer.net.layers.3.0.fn.to_out.weight', 'model.decoder.performer.net.layers.3.0.fn.to_out.bias', 'model.decoder.performer.net.layers.3.1.norm.weight', 'model.decoder.performer.net.layers.3.1.norm.bias', 'model.decoder.performer.net.layers.3.1.fn.fn.w1.weight', 'model.decoder.performer.net.layers.3.1.fn.fn.w1.bias', 'model.decoder.performer.net.layers.3.1.fn.fn.w2.weight', 'model.decoder.performer.net.layers.3.1.fn.fn.w2.bias', 'model.decoder.performer.net.layers.4.0.norm.weight', 'model.decoder.performer.net.layers.4.0.norm.bias', 'model.decoder.performer.net.layers.4.0.fn.fast_attention.projection_matrix', 'model.decoder.performer.net.layers.4.0.fn.to_q.weight', 'model.decoder.performer.net.layers.4.0.fn.to_q.bias', 'model.decoder.performer.net.layers.4.0.fn.to_k.weight', 'model.decoder.performer.net.layers.4.0.fn.to_k.bias', 'model.decoder.performer.net.layers.4.0.fn.to_v.weight', 'model.decoder.performer.net.layers.4.0.fn.to_v.bias', 'model.decoder.performer.net.layers.4.0.fn.to_out.weight', 'model.decoder.performer.net.layers.4.0.fn.to_out.bias', 'model.decoder.performer.net.layers.4.1.norm.weight', 'model.decoder.performer.net.layers.4.1.norm.bias', 'model.decoder.performer.net.layers.4.1.fn.fn.w1.weight', 'model.decoder.performer.net.layers.4.1.fn.fn.w1.bias', 'model.decoder.performer.net.layers.4.1.fn.fn.w2.weight', 'model.decoder.performer.net.layers.4.1.fn.fn.w2.bias', 'model.decoder.performer.net.layers.5.0.norm.weight', 'model.decoder.performer.net.layers.5.0.norm.bias', 'model.decoder.performer.net.layers.5.0.fn.fast_attention.projection_matrix', 'model.decoder.performer.net.layers.5.0.fn.to_q.weight', 'model.decoder.performer.net.layers.5.0.fn.to_q.bias', 'model.decoder.performer.net.layers.5.0.fn.to_k.weight', 'model.decoder.performer.net.layers.5.0.fn.to_k.bias', 'model.decoder.performer.net.layers.5.0.fn.to_v.weight', 'model.decoder.performer.net.layers.5.0.fn.to_v.bias', 'model.decoder.performer.net.layers.5.0.fn.to_out.weight', 'model.decoder.performer.net.layers.5.0.fn.to_out.bias', 'model.decoder.performer.net.layers.5.1.norm.weight', 'model.decoder.performer.net.layers.5.1.norm.bias', 'model.decoder.performer.net.layers.5.1.fn.fn.w1.weight', 'model.decoder.performer.net.layers.5.1.fn.fn.w1.bias', 'model.decoder.performer.net.layers.5.1.fn.fn.w2.weight', 'model.decoder.performer.net.layers.5.1.fn.fn.w2.bias', 'model.decoder.norm.weight', 'model.decoder.norm.bias'])
****************************************************************************************************

finetuned_model318.ckpt["gene"]
1****************************************************************************************************
Gene keys: dict_keys(['config', 'state_dict'])
2****************************************************************************************************
Gene Config: {'mask_gene_name': False, 'gene_num': 19266, 'seq_len': 19266, 'encoder': {'hidden_dim': 768, 'depth': 12, 'heads': 12, 'dim_head': 64, 'seq_len': 19266, 'module_type': 'transformer', 'norm_first': False}, 'decoder': {'hidden_dim': 512, 'depth': 6, 'heads': 8, 'dim_head': 64, 'module_type': 'performer', 'seq_len': 19266, 'norm_first': False}, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102, 'bin_num': 100, 'bin_alpha': 1.0, 'rawcount': True, 'model': 'mae_autobin', 'test_valid_train_idx_dict': '/nfs_beijing/minsheng/data/os10000w-new/global_shuffle/meta.csv.train_set_idx_dict.pt', 'valid_data_path': '/nfs_beijing/minsheng/data/valid_count_10w.npz', 'num_tokens': 13, 'train_data_path': None, 'isPanA': False, 'isPlanA1': False, 'max_files_to_load': 5, 'bin_type': 'auto_bin', 'value_mask_prob': 0.3, 'zero_mask_prob': 0.03, 'replace_prob': 0.8, 'random_token_prob': 0.1, 'mask_ignore_token_ids': [0], 'decoder_add_zero': True, 'mae_encoder_max_seq_len': 15000, 'isPlanA': False, 'mask_prob': 0.3, 'model_type': 'mae_autobin', 'pos_embed': False, 'device': 'cuda', 'ppi_edge': None}
3****************************************************************************************************
Gene State Dict Keys: odict_keys(['token_emb.mlp.weight', 'token_emb.mlp.bias', 'token_emb.mlp2.weight', 'token_emb.mlp2.bias', 'token_emb.emb.weight', 'token_emb.emb_mask.weight', 'token_emb.emb_pad.weight', 'pos_emb.weight', 'encoder.transformer_encoder.0.self_attn.in_proj_weight', 'encoder.transformer_encoder.0.self_attn.in_proj_bias', 'encoder.transformer_encoder.0.self_attn.out_proj.weight', 'encoder.transformer_encoder.0.self_attn.out_proj.bias', 'encoder.transformer_encoder.0.linear1.weight', 'encoder.transformer_encoder.0.linear1.bias', 'encoder.transformer_encoder.0.linear2.weight', 'encoder.transformer_encoder.0.linear2.bias', 'encoder.transformer_encoder.0.norm1.weight', 'encoder.transformer_encoder.0.norm1.bias', 'encoder.transformer_encoder.0.norm2.weight', 'encoder.transformer_encoder.0.norm2.bias', 'encoder.transformer_encoder.1.self_attn.in_proj_weight', 'encoder.transformer_encoder.1.self_attn.in_proj_bias', 'encoder.transformer_encoder.1.self_attn.out_proj.weight', 'encoder.transformer_encoder.1.self_attn.out_proj.bias', 'encoder.transformer_encoder.1.linear1.weight', 'encoder.transformer_encoder.1.linear1.bias', 'encoder.transformer_encoder.1.linear2.weight', 'encoder.transformer_encoder.1.linear2.bias', 'encoder.transformer_encoder.1.norm1.weight', 'encoder.transformer_encoder.1.norm1.bias', 'encoder.transformer_encoder.1.norm2.weight', 'encoder.transformer_encoder.1.norm2.bias', 'encoder.transformer_encoder.2.self_attn.in_proj_weight', 'encoder.transformer_encoder.2.self_attn.in_proj_bias', 'encoder.transformer_encoder.2.self_attn.out_proj.weight', 'encoder.transformer_encoder.2.self_attn.out_proj.bias', 'encoder.transformer_encoder.2.linear1.weight', 'encoder.transformer_encoder.2.linear1.bias', 'encoder.transformer_encoder.2.linear2.weight', 'encoder.transformer_encoder.2.linear2.bias', 'encoder.transformer_encoder.2.norm1.weight', 'encoder.transformer_encoder.2.norm1.bias', 'encoder.transformer_encoder.2.norm2.weight', 'encoder.transformer_encoder.2.norm2.bias', 'encoder.transformer_encoder.3.self_attn.in_proj_weight', 'encoder.transformer_encoder.3.self_attn.in_proj_bias', 'encoder.transformer_encoder.3.self_attn.out_proj.weight', 'encoder.transformer_encoder.3.self_attn.out_proj.bias', 'encoder.transformer_encoder.3.linear1.weight', 'encoder.transformer_encoder.3.linear1.bias', 'encoder.transformer_encoder.3.linear2.weight', 'encoder.transformer_encoder.3.linear2.bias', 'encoder.transformer_encoder.3.norm1.weight', 'encoder.transformer_encoder.3.norm1.bias', 'encoder.transformer_encoder.3.norm2.weight', 'encoder.transformer_encoder.3.norm2.bias', 'encoder.transformer_encoder.4.self_attn.in_proj_weight', 'encoder.transformer_encoder.4.self_attn.in_proj_bias', 'encoder.transformer_encoder.4.self_attn.out_proj.weight', 'encoder.transformer_encoder.4.self_attn.out_proj.bias', 'encoder.transformer_encoder.4.linear1.weight', 'encoder.transformer_encoder.4.linear1.bias', 'encoder.transformer_encoder.4.linear2.weight', 'encoder.transformer_encoder.4.linear2.bias', 'encoder.transformer_encoder.4.norm1.weight', 'encoder.transformer_encoder.4.norm1.bias', 'encoder.transformer_encoder.4.norm2.weight', 'encoder.transformer_encoder.4.norm2.bias', 'encoder.transformer_encoder.5.self_attn.in_proj_weight', 'encoder.transformer_encoder.5.self_attn.in_proj_bias', 'encoder.transformer_encoder.5.self_attn.out_proj.weight', 'encoder.transformer_encoder.5.self_attn.out_proj.bias', 'encoder.transformer_encoder.5.linear1.weight', 'encoder.transformer_encoder.5.linear1.bias', 'encoder.transformer_encoder.5.linear2.weight', 'encoder.transformer_encoder.5.linear2.bias', 'encoder.transformer_encoder.5.norm1.weight', 'encoder.transformer_encoder.5.norm1.bias', 'encoder.transformer_encoder.5.norm2.weight', 'encoder.transformer_encoder.5.norm2.bias', 'encoder.transformer_encoder.6.self_attn.in_proj_weight', 'encoder.transformer_encoder.6.self_attn.in_proj_bias', 'encoder.transformer_encoder.6.self_attn.out_proj.weight', 'encoder.transformer_encoder.6.self_attn.out_proj.bias', 'encoder.transformer_encoder.6.linear1.weight', 'encoder.transformer_encoder.6.linear1.bias', 'encoder.transformer_encoder.6.linear2.weight', 'encoder.transformer_encoder.6.linear2.bias', 'encoder.transformer_encoder.6.norm1.weight', 'encoder.transformer_encoder.6.norm1.bias', 'encoder.transformer_encoder.6.norm2.weight', 'encoder.transformer_encoder.6.norm2.bias', 'encoder.transformer_encoder.7.self_attn.in_proj_weight', 'encoder.transformer_encoder.7.self_attn.in_proj_bias', 'encoder.transformer_encoder.7.self_attn.out_proj.weight', 'encoder.transformer_encoder.7.self_attn.out_proj.bias', 'encoder.transformer_encoder.7.linear1.weight', 'encoder.transformer_encoder.7.linear1.bias', 'encoder.transformer_encoder.7.linear2.weight', 'encoder.transformer_encoder.7.linear2.bias', 'encoder.transformer_encoder.7.norm1.weight', 'encoder.transformer_encoder.7.norm1.bias', 'encoder.transformer_encoder.7.norm2.weight', 'encoder.transformer_encoder.7.norm2.bias', 'encoder.transformer_encoder.8.self_attn.in_proj_weight', 'encoder.transformer_encoder.8.self_attn.in_proj_bias', 'encoder.transformer_encoder.8.self_attn.out_proj.weight', 'encoder.transformer_encoder.8.self_attn.out_proj.bias', 'encoder.transformer_encoder.8.linear1.weight', 'encoder.transformer_encoder.8.linear1.bias', 'encoder.transformer_encoder.8.linear2.weight', 'encoder.transformer_encoder.8.linear2.bias', 'encoder.transformer_encoder.8.norm1.weight', 'encoder.transformer_encoder.8.norm1.bias', 'encoder.transformer_encoder.8.norm2.weight', 'encoder.transformer_encoder.8.norm2.bias', 'encoder.transformer_encoder.9.self_attn.in_proj_weight', 'encoder.transformer_encoder.9.self_attn.in_proj_bias', 'encoder.transformer_encoder.9.self_attn.out_proj.weight', 'encoder.transformer_encoder.9.self_attn.out_proj.bias', 'encoder.transformer_encoder.9.linear1.weight', 'encoder.transformer_encoder.9.linear1.bias', 'encoder.transformer_encoder.9.linear2.weight', 'encoder.transformer_encoder.9.linear2.bias', 'encoder.transformer_encoder.9.norm1.weight', 'encoder.transformer_encoder.9.norm1.bias', 'encoder.transformer_encoder.9.norm2.weight', 'encoder.transformer_encoder.9.norm2.bias', 'encoder.transformer_encoder.10.self_attn.in_proj_weight', 'encoder.transformer_encoder.10.self_attn.in_proj_bias', 'encoder.transformer_encoder.10.self_attn.out_proj.weight', 'encoder.transformer_encoder.10.self_attn.out_proj.bias', 'encoder.transformer_encoder.10.linear1.weight', 'encoder.transformer_encoder.10.linear1.bias', 'encoder.transformer_encoder.10.linear2.weight', 'encoder.transformer_encoder.10.linear2.bias', 'encoder.transformer_encoder.10.norm1.weight', 'encoder.transformer_encoder.10.norm1.bias', 'encoder.transformer_encoder.10.norm2.weight', 'encoder.transformer_encoder.10.norm2.bias', 'encoder.transformer_encoder.11.self_attn.in_proj_weight', 'encoder.transformer_encoder.11.self_attn.in_proj_bias', 'encoder.transformer_encoder.11.self_attn.out_proj.weight', 'encoder.transformer_encoder.11.self_attn.out_proj.bias', 'encoder.transformer_encoder.11.linear1.weight', 'encoder.transformer_encoder.11.linear1.bias', 'encoder.transformer_encoder.11.linear2.weight', 'encoder.transformer_encoder.11.linear2.bias', 'encoder.transformer_encoder.11.norm1.weight', 'encoder.transformer_encoder.11.norm1.bias', 'encoder.transformer_encoder.11.norm2.weight', 'encoder.transformer_encoder.11.norm2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'fc1.0.weight', 'fc1.0.bias', 'fc1.2.weight', 'fc1.2.bias', 'norm.running_mean', 'norm.running_var', 'norm.num_batches_tracked'])
****************************************************************************************************